# Sample sales data from Kaggle: https://www.kaggle.com/datasets/kyanyoga/sample-sales-data
# File location and type
#First row is header, spark will allocate some default column names
file_location = "/FileStore/tables/spark_modeling/salesData/sales_06_FY2020_21_copy.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
.option("inferSchema", infer_schema) \
.option("header", first_row_is_header) \
.option("sep", delimiter) \
.load(file_location)

display(df)

# Check data schema
df.printSchema()


### Create temporary table or view
temp_table_name = "sales_06_FY2020_21_copy_csv"
df.createOrReplaceTempView(temp_table_name)

### Use SQL SELECT * command to check the dataframe
%sql
select * from `sales_06_FY2020_21_copy_csv`

### Create a permanent table name so that data can persist on Databricks HDFS
permanent_table_name = "sales_06_FY2020_21_copy_csv_v2"

#### Persist the data
df.write.format("parquet").saveAsTable(permanent_table_name)

### Data can be converted to other formats for modeling. In this instance, the data is converted to #Python list format
df.toPandas().values.tolist()

### Convert data to dictionary or JSON format
df.toPandas().to_dict('records')

## Compare Pyspark's show() method with Databrick's display method 
## show() method
df2 = df
df2.show()

#### Display method
display(df2)

#### show all the data columns
df2.columns

### show  the data colums with their data types
df2.dtypes

### drop some dataframe columns
col_to_be_dropped = ['sku' , 'category', 'payment', 'bi_st', 'cust_id' , 'ref_num', 'Name Prefix', 'E mail', 'Customer Since', 'SSN', 'Phone No.' , 'Place Name', 'County', 'City', 'State', 'Zip', 'Region', 'User Name' ]
df3 = df2.drop(*col_to_be_dropped)
df3.display()


##convert gender column to format 0, 1 instead of 'M' , 'F'. Write the result to a new dataframe
df4 = df3.na.replace(['M','F'],['1','0'])
df4.display()

### Rename some columns and map it to previous columns
new_col_names = {'First Name':'first_name','Middle Initial':'middle_initial', 'Last Name': 'last_name'}
new_names = [new_col_names.get(col,col) for col in df3.columns]
df4 = df3.toDF(*new_names)
df4.display()


## use 'withColumnRenamed' command to create a new column 'discount(%)'
df5 = df4.withColumnRenamed('Discount_Percent','discount(%)')
df5.display()

### Filter data to show where age < 60
df5[df5.age<60].display()

### Filter data to show where age > 70
df5[df5.age>70].display()

### Show distinct row of names
df5.select("full_name").distinct().display()

### Use 'dropDuplicates' command to show distinct row of names
df5.dropDuplicates(["full_name"]).display()

### Create index column
from pyspark.sql.functions import row_number
from pyspark.sql.window import Window
df5_index = df5.withColumn("index", row_number().over(Window.orderBy("full_name")))
df5_index.display()


##### Filter data between some age range
age_51_to_70 = df5[(df5.age < 70)&(df5.age > 50)].display()


##### Normalize price column by dividing each price row with the sum of all item prices
from pyspark.sql import functions as F
df5.withColumn('normalized_price',df5.price/df5.groupBy().agg(F.sum("price")).collect()[0][0]).display()

#### Create a new condition column based on some criteria
# When age is > 50 and < 10, the condition column should display 1, 
# When quantity ordered is greater than 5, condition column should display 2
# Otherwise, condition column should display 3

df5.withColumn('condition_column',F.when((df5.age>50)&(df5.age<70),1)\
.when(df5.qty_ordered>5,2)\
.otherwise(3)).display()

### Create a natural logarithm column
df5.withColumn('log_total',F.log(df5.total)).display()

#### Stop your spark session
spark.stop()


















